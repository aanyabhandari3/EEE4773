{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 - Code Implementation for the Naive Bayes Classifier & Its Discriminant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import scipy.stats as stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    data_C1 = stats.multivariate_normal(mean1, cov1).rvs(size=N1)\n",
    "    data_C2 = stats.multivariate_normal(mean2, cov2).rvs(size=N2)\n",
    "        \n",
    "    # Entire Training Dataset\n",
    "    data = np.concatenate((data_C1, data_C2))\n",
    "    labels = np.concatenate((np.ones(N1),2*np.ones(N2)))\n",
    "    \n",
    "    if len(mean1)>1:\n",
    "        plt.scatter(data[labels==1,0], data[labels==1,1], c='b', alpha=0.5, edgecolors='k')\n",
    "        plt.scatter(data[labels==2,0], data[labels==2,1], c='r', alpha=0.5, edgecolors='k')\n",
    "        plt.xlabel('Feature 1'); plt.ylabel('Feature 2');\n",
    "    else:\n",
    "        plt.scatter(data[labels==1], np.ones(N1), c='b', alpha=0.5, edgecolors='k')\n",
    "        plt.scatter(data[labels==2], np.ones(N2), c='r', alpha=0.5, edgecolors='k')\n",
    "        plt.xlabel('Feature 1');\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Univariate Data Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-2]\n",
    "mean2 = [1]\n",
    "var1 = [1]\n",
    "var2 = [2]\n",
    "N1 = 50 # C1 - blue\n",
    "N2 = 100 # C2 - red\n",
    "\n",
    "data, labels = generateData(mean1, mean2, var1, var2, N1, N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMAP(data, labels):\n",
    "    \n",
    "    #### Estimate parameters (MLE solution)\n",
    "    # Means\n",
    "    mu1 = np.mean(data[labels==1]) \n",
    "    mu2 = np.mean(data[labels==2])\n",
    "    \n",
    "    # Variances\n",
    "    var1 = np.cov(data[labels==1])\n",
    "    var2 = np.cov(data[labels==2])\n",
    "    \n",
    "    #### Estimate Prior Probabilities - relative frequency\n",
    "    N = len(data)\n",
    "    N1 = np.sum(labels==1)\n",
    "    N2 = N - N1\n",
    "    p1 = N1/N # prior probability for C1\n",
    "    p2 = N2/N # prior probability for C2\n",
    "    \n",
    "    #### Define data likelihoods \n",
    "    G1=stats.norm(loc=mu1,scale=np.sqrt(var1)) # P(x|C1)\n",
    "    G2=stats.norm(loc=mu2,scale=np.sqrt(var2)) # P(x|C2)\n",
    "    x=np.linspace(-6,6,1001)\n",
    "        \n",
    "    #### Plot the weighted densities\n",
    "    # these are proportional to the posteriors\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(x,p1*G1.pdf(x),label='$f_X(x|C_1)P(C_1)$')\n",
    "    plt.plot(x,p2*G2.pdf(x),label='$f_X(x|C_2)P(C_2)$')\n",
    "    \n",
    "    #### Determine the regions where the posterior for deciding C1 \n",
    "    # and the posterior for deciding C2\n",
    "    R1=x[np.where(p1*G1.pdf(x)>= p2*G2.pdf(x))]\n",
    "    R2=x[np.where(p1*G1.pdf(x)< p2*G2.pdf(x))]\n",
    "\n",
    "    # Fill under the regions found above\n",
    "    plt.fill_between(R1,p1*G1.pdf(R1),alpha=0.3,label='Decide C1')\n",
    "    plt.fill_between(R2,p2*G2.pdf(R2),alpha=0.3,label='Decide C2')\n",
    "    plt.scatter(data[labels==1], -0.01*np.ones(N1), c='b', alpha=0.5, edgecolors='k')\n",
    "    plt.scatter(data[labels==2], -0.01*np.ones(N2), c='r', alpha=0.5, edgecolors='k')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print the MAP threshold\n",
    "    print('MAP decision threshold to decide C2 is >',round(R2[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMAP(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Multivariate Data Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "data, labels = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[labels==1,0], data[labels==1,1], color='red', label='C1')\n",
    "plt.scatter(data[labels==2,0], data[labels==2,1], color='blue', label='C2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Estimate parameters (with MLE solutions)\n",
    "# Means\n",
    "mu1 = np.mean(data[labels==1], axis=0)\n",
    "print('Mean of Class 1: ', mu1)\n",
    "mu2 = np.mean(data[labels==2], axis=0)\n",
    "print('Mean of Class 2: ', mu2)\n",
    "\n",
    "# Covariances - in this example we are showing the case where we estimate the full covariance\n",
    "cov1 = np.cov(data[labels==1,:].T) # np.cov expects input to be D-by-N\n",
    "print('Covariance of Class 1: ',cov1)\n",
    "cov2 = np.cov(data[labels==2, :].T)\n",
    "print('Covariance of Class 2: ',cov2)\n",
    "\n",
    "#### Estimate Prior Probabilities\n",
    "N = N1+N2\n",
    "p1 = N1/N\n",
    "print('Probability of  Class 1: ',p1)\n",
    "p2 = N2/N\n",
    "print('Probability of  Class 2: ',p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "grid = 4\n",
    "x = np.linspace(-grid, grid, 100)\n",
    "y = np.linspace(-grid, grid, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0) # grid of values\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "y1 = stats.multivariate_normal.pdf(X, mean=mu1, cov=cov1) #P(x|C1) - data likelihood for C1\n",
    "y2 = stats.multivariate_normal.pdf(X, mean=mu2, cov=cov2) #P(x|C2)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(data[labels==1,0], data[labels==1,1], c='r',alpha=0.3)\n",
    "plt.imshow(y1, extent=[-grid,grid,-grid,grid],cmap='YlOrRd')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(data[labels==2,0], data[labels==2,1], c='b',alpha=0.3)\n",
    "plt.imshow(y2, extent=[-grid,grid,-grid,grid], cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "pos1 = (y1*p1)/(y1*p1+y2*p2) # P(C1|x) - posterior probability\n",
    "pos2 = (y2*p2)/(y1*p1+y2*p2) # P(C2|x)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(pos1, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(pos2, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision boundary for deciding Class 2\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos2>pos1, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use this classifier to predict the class label for point $[1,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,1]\n",
    "\n",
    "# Data Likelihoods\n",
    "y1_newPoint = stats.multivariate_normal.pdf(x, mean=mu1, cov=cov1) #P(x|C1)\n",
    "y2_newPoint = stats.multivariate_normal.pdf(x, mean=mu2, cov=cov2) #P(x|C2)\n",
    "\n",
    "print('Data likelihoods:')\n",
    "print('P(x|C1) = ', y1_newPoint)\n",
    "print('P(x|C2) = ', y2_newPoint,'\\n')\n",
    "\n",
    "# Posterior Probabilities\n",
    "y1_pos = (y1_newPoint*p1)/(y1_newPoint*p1+y2_newPoint*p2) #P(C1|x)\n",
    "y2_pos = (y2_newPoint*p2)/(y1_newPoint*p1+y2_newPoint*p2) #P(C2|x)\n",
    "\n",
    "print('Posterior probabilities:')\n",
    "print('P(C1|x) = ', y1_pos)\n",
    "print('P(C2|x) = ', y2_pos,'\\n')\n",
    "\n",
    "if y1_pos > y2_pos:\n",
    "    print('x = ',x,' belongs to class 1')\n",
    "else:\n",
    "    print('x = ',x,' belongs to class 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about $x=[4,4]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4,4]\n",
    "\n",
    "# Data Likelihoods\n",
    "y1_newPoint = stats.multivariate_normal.pdf(x, mean=mu1, cov=cov1) #P(x|C1)\n",
    "y2_newPoint = stats.multivariate_normal.pdf(x, mean=mu2, cov=cov2) #P(x|C2)\n",
    "\n",
    "print('Data likelihoods:')\n",
    "print('P(x|C1) = ', y1_newPoint)\n",
    "print('P(x|C2) = ', y2_newPoint,'\\n')\n",
    "\n",
    "# Posterior Probabilities\n",
    "y1_pos = (y1_newPoint*p1)/(y1_newPoint*p1+y2_newPoint*p2) #P(C1|x)\n",
    "y2_pos = (y2_newPoint*p2)/(y1_newPoint*p1+y2_newPoint*p2) #P(C2|x)\n",
    "\n",
    "print('Posterior probabilities:')\n",
    "print('P(C1|x) = ', y1_pos)\n",
    "print('P(C2|x) = ', y2_pos,'\\n')\n",
    "\n",
    "if y1_pos > y2_pos:\n",
    "    print('x = ',x,' belongs to class 1')\n",
    "else:\n",
    "    print('x = ',x,' belongs to class 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification can also be seen as implementing a set of **discriminant functions**, $g_i(x), i=1,\\dots, K$, such that we\n",
    "\n",
    "$$\\text{Choose} \\;\\; C_i \\;\\; \\text{if} \\;\\; g_i(x) = \\max_k g_k(x)$$\n",
    "\n",
    "where $g_i(\\mathbf{x}) = \\ln(P(C_i|\\mathbf{x})) = \\ln(P(\\mathbf{x}|C_i)P(C_i))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **two classes** ($C_1$ and $C_2$), we have the **Bayesian decision rule**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & P(C_1|x) > P(C_2|x) \\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & P(x|C_1)P(C_1) > P(x|C_2)P(C_2)\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & \\ln(P(x|C_1)P(C_1)) > \\ln(P(x|C_2)P(C_2))\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & g_1(x) > g_2(x)\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & g_1(x)-g_2(x) > 0\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **two classes**, we can define a single discriminant\n",
    "\n",
    "$$g(\\mathbf{x}) = g_1(\\mathbf{x}) - g_2(\\mathbf{x})$$\n",
    "\n",
    "and we\n",
    "\n",
    "$$\\text{Choose} \\begin{cases}C_1 & \\text{if} \\; g(\\mathbf{x})>0\\\\ C_2 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly calculate the decision boundary for the two-class two-dimensional data. Assume that the data likelihood for each class is a bivariate Gaussian distribution\n",
    "\n",
    "$$P(\\mathbf{x}|C_i) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma_i|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mu_i)^T\\Sigma_i^{-1}(\\mathbf{x}-\\mu_i)\\right\\}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mu_1 =\\begin{bmatrix}3\\\\6\\end{bmatrix}, \\;\\;\\; \\mu_2 =\\begin{bmatrix}3\\\\-2\\end{bmatrix}, \\;\\;\\; \\Sigma_1=\\begin{bmatrix}1/2 & 0\\\\0 &2\\end{bmatrix}, \\;\\;\\; \\Sigma_2=\\begin{bmatrix}2 & 0\\\\0 &2\\end{bmatrix}$$\n",
    "\n",
    "The inverse matrices are\n",
    "\n",
    "$$\\Sigma_1^{-1}=\\begin{bmatrix}2 & 0\\\\0 &1/2\\end{bmatrix}, \\;\\;\\; \\Sigma_2^{-1}=\\begin{bmatrix}1/2 & 0\\\\0 &1/2\\end{bmatrix}$$\n",
    "\n",
    "Assume equal prior probabilities $P(C_1)=P(C_2)=\\frac{1}{2}$.\n",
    "\n",
    "1. Compute the discriminant function (decision function).\n",
    "\n",
    "**Answer in board notes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = [3, 6]\n",
    "mu2 = [3, -2]\n",
    "\n",
    "Sigma1 = np.array([[0.5,0],[0,2]])\n",
    "Sigma2 = np.array([[2,0],[0,2]])\n",
    "\n",
    "p1 = 0.5\n",
    "p2 = 1-p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "gridx = 15\n",
    "gridy = 20\n",
    "x = np.linspace(-gridx, gridx, 500)\n",
    "y = np.linspace(-10, gridy, 500)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0) # grid of values\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "y1 = stats.multivariate_normal.pdf(X, mean=mu1, cov=Sigma1) #P(x|C1) - data likelihood for C1\n",
    "y2 = stats.multivariate_normal.pdf(X, mean=mu2, cov=Sigma2) #P(x|C2)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "pos1 = (y1*p1)/(y1*p1 + y2*p2) # P(C1|x) - posterior probability\n",
    "pos2 = (y2*p2)/(y1*p1 + y2*p2) # P(C2|x)\n",
    "\n",
    "# Look at the decision boundary:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos1>pos2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-6.8,12.8, 100)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos1>pos2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.plot(x1, 3.514 - 1.125*x1 + 0.1875*x1**2, 'r', label='Discriminant function')\n",
    "plt.colorbar(); plt.legend()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if the data likelihood looks like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, t = make_blobs(n_samples=200, centers=5, n_features=2)\n",
    "plt.scatter(X[:,0], X[:,1],color='blue', alpha=0.5, edgecolor='k')\n",
    "plt.xlabel('Feature $x_1$',size=15); plt.ylabel('Feature $x_2$',size=15);\n",
    "plt.title('Data likelihood $f_X([x_1,x_2])$',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a single Gaussian distribution, we would obtain a very poor estimate of the true underlying data likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Supervised Learning. Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <strong>Supervised Learning</strong>\n",
    "    \n",
    "Learning a mapping from input data to desired output values given labeled training data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning performs 2 different types of tasks: \n",
    "1. Classification\n",
    "2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification** is a form of predictive modeling approach to characterize the relationship between some collection of observational input data and a set of categorical labels.\n",
    "\n",
    "Suppose we have training images from two classes, class 0 is macaw and class 1 is conure, and we would like to train a classifier to assign a label to incoming test images whether they belong to class class 0 or class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.022Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/classification.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **classification** example. Each data point was classified into a **discrete class** (either conure or macaw). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers can further be sub-categorized as **discriminative** or **generative** classifiers.\n",
    "\n",
    "* A **discriminative** approach for classification is one in which we partition the feature space into regions for each class. Then, when we have a test point, we evaluate in which region it landed on and classify it accordingly.\n",
    "\n",
    "* A **generative** approach for classification is one in which we estimate the parameters for distributions that generate the data for each class using Bayesian principles. When we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression** is a form of *predictive modeling* approach to characterize the relationship between some collection of observational input data and a continuous desired response. \n",
    "\n",
    "* A linear regression model is a linear combination of input values.\n",
    "\n",
    "Consider the example below:\n",
    "\n",
    "* The goal is to *train* a model that takes in the silhouette images with their correspondent labels (age of the person in the silhouette) and learn a linear relationship between images and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.031Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('figures/regression.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After the model is trained, the **goal** is to be able to *predict* the desired output value of any *new* unlabeled test data.\n",
    "    \n",
    "Applications of regression include: electric/solar power forecast, stock market, inventory investment, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Flowchart for Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual flow (but not always) for supervised learning is:\n",
    "* **Training stage**\n",
    "    1. Collect labeled training data - often the most time-consuming and expensive task. This constitutes the **input space**.\n",
    "    2. Extract features - extract *useful* features from the input (or observational) data such that they have discriminatory information in successfully mapping the desired output. This constitutes the **feature space**.\n",
    "    3. Select a model - relationship between input data and desired output.\n",
    "    4. Fit the model - change model parameters (**Learning Algorithm**) in order to meet some **Objective Function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.038Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('figures/training.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Testing:**\n",
    "    1. Given unlabeled test data\n",
    "    2. Extract (the same) features\n",
    "    3. Run the unlabeled data through the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.040Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('figures/testing.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a Supervised ML System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the *virtual whiteboard* to describe the steps of \"fitting a model\".\n",
    "\n",
    "From the whiteboard notes, we saw the flowchart for Supervised Learning.\n",
    "\n",
    "(See whiteboard notes) The system has a **feedback** loop which will make this approach completely **autonomous** without user intervention.\n",
    "\n",
    "* But yet we have fully control of each component's design choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the challenges of supervised learning include:\n",
    "\n",
    "* How do you know if you have *representative* training data?\n",
    "* How do you know if you extracted *good* features?\n",
    "* How do you know if you selected the *right* model?\n",
    "* How do you know if you selected the *right* objective function?\n",
    "* How do you know if you trained the model *well*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these challenges are alleviated (not solved entirely, but helped significantly) with *LOTS AND LOTS* of **data** and a good **experimental design**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.049Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('figures/PHDcomics.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, sometimes, obtaining labeled training data is hard, expensive, time consuming and, in some cases, infeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.051Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('figures/NEON.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Polynomial Features (or *Polynomial Regression*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by considering the polynomial curve fitting example in the first chapter of the Bishop textbook.\n",
    "\n",
    "* A regression task is a supervised learning task.\n",
    "* The target labels are continuous.\n",
    "* Linear regression is *linear* on the parameters, that is, linear regression is a linear combination of features (which may not be linear themselves).\n",
    "* If the features are extracted with polynomial basis function, we call the linear regression model a **polynomial regression** model.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Polynomial Regression** is a type of liner regression that uses a special set of *features* - polynomial features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 1 - Input Space</b> \n",
    "\n",
    "Suppose we are given a training set comprising of $N$ observations of $\\mathbf{x}$, $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$, and its corresponding target outputs $\\mathbf{t} = \\left[t_1, t_2, \\ldots, t_N\\right]^T$, where sample $x_i$ has the target output label $t_i$.\n",
    "\n",
    "So, we want to learn the *true* function mapping $f$ such that $\\mathbf{t}  = f(\\mathbf{x}, \\mathbf{w})$, where $\\mathbf{w}$ are parameters of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that both the training data and target outputs can be noisy. Sometimes the target outputs can be mislabeled, so it is important to apply \n",
    "\n",
    "* We generally organize data into *matrices* and *vectors*. Not only is it a common way to organize the data, but it allows us to easily apply algebraic operations during analysis. It also makes it much simpler when it comes to code implementation!\n",
    "    * In most engineering textbooks and in this course, **vectors** are defined as *column vectors*. This is why we write $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 2 - Feature Extraction</b> \n",
    "\n",
    "Let's consider *polynomial features* for each data point $x_i$, $\\phi(x_i)$. We define $\\phi(\\cdot)$ as the **polynomial basis function** of order $M$: $\\phi(x_i) = \\left[x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^M\\right]^T$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Other features can be extracted. These can be other basis functions, but can also be extracted by constructing custom coding scripts. Other **basis functions** include:\n",
    "\n",
    "    * Polynomials Basis functions: $\\phi_j(x) = x^j$\n",
    "\n",
    "    * Radial Basis functions: $\\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2)}{2s^2}\\right\\}$\n",
    "\n",
    "    * Sigmoidal basis function: $\\phi_j(x) = \\sigma\\left(\\frac{x-\\mu_j}{s}\\right)$, where $\\sigma(a)$ is the logistic sigmoid function defined by $\\sigma(a)=\\frac{1}{1+\\exp(-a)}$\n",
    "\n",
    "    * Fourier Basis functions, which leads to an expansion in sinusoidal functions. Each basis function represents a specific frequency and has infinite spatial extent\n",
    "\n",
    "    * Wavelets Basis Functions, representing both space and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basis Functions\n",
    "x=np.linspace(-1,1,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,3,1)\n",
    "polynomials=np.array([x**i for i in range(1,10)]).T # Polynomial Basis Functions\n",
    "plt.plot(x, polynomials); plt.title('Polynomial Basis Functions')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sig=0.3 #fixed standard deviation for each Gaussian basis function\n",
    "polynomials=np.array([np.exp(-(x-i)**2/(2*sig**2)) for i in np.linspace(-1,1,10)]).T # Gaussian Basis Functions\n",
    "plt.plot(x, polynomials); plt.title('Gaussian Basis Functions')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sig=0.05 #fixed standard deviation for each sigmoidal basis function\n",
    "polynomials=np.array([1/(1+np.exp(-(x-i)/sig)) for i in np.linspace(-1,1,10)]).T # Sigmoidal Basis Functions\n",
    "plt.plot(x, polynomials); plt.title('Sigmoidal Basis Functions')\n",
    "\n",
    "legend_labels=['$\\phi_0(x)$','$\\phi_1(x)$','$\\phi_2(x)$','$\\phi_3(x)$','$\\phi_4(x)$','$\\phi_5(x)$',\n",
    "           '$\\phi_6(x)$','$\\phi_7(x)$','$\\phi_8(x)$','$\\phi_9(x)$','$\\phi_{10}(x)$']\n",
    "fig.legend(legend_labels, loc = (0.1, 0), ncol=11, fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature Space</b> \n",
    "\n",
    "The set of features drawn by the transformation \n",
    "\n",
    "\\begin{align*}\n",
    "\\phi: \\mathbb{R} & \\rightarrow \\mathbb{R}^{M+1} \\\\\n",
    "x & \\rightarrow [\\phi_0(x), \\phi_1(x), \\phi_2(x), ..., \\phi_M(x)]\n",
    "\\end{align*}\n",
    "is often called the **feature space**.\n",
    "When we write a linear regression with respect to a set of basis functions, the regression model is linear in the *feature space*.\n",
    "\n",
    "In this case, the dimensionality of the feature space is $M+1$. This is often referred to as the *model order*.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **feature matrix** $\\mathbf{X}$ is a matrix containing all feature mappings for all samples $\\{x_i\\}_{i=1}^N$ and can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times (M+1)}\n",
    "\\end{align*}\n",
    "\n",
    "where each row is a feature representation of a data point $x_i$.\n",
    "\n",
    "* Our goal is to find the mapping from the feature input data $\\mathbf{X}$ to the desired output values $\\mathbf{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the data actually comes from some **unknown hidden function**, that takes in the data points $\\mathbf{x}$ with some parameters $\\mathbf{w}$ and produces the desired values $\\mathbf{t}$, i.e. $\\mathbf{t} = f(\\mathbf{x},\\mathbf{w})$.\n",
    "* We do not know anything about the function $f$. If we knew the hidden function, we would not need to learn the *mapping* - we would already know it. However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 3 - Model Selection or Mapping</b> \n",
    "\n",
    "Let's assume that the desired output values are a *linear combination* of the feature input space, i.e., the **polynomial function**\n",
    "\n",
    "\\begin{align*}\n",
    "t \\sim y(x,\\mathbf{w}) = w_0x^0 + w_1x^1 + w_2x^2+\\cdots+w_Mx^M = \\sum_{j=0}^{M} w_jx^j = \\mathbf{X}\\mathbf{w}\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "* This means that for every paired training data point $\\{(x_i, t_i)\\}_{i=1}^N$, we can model the output value as \n",
    "\n",
    "\\begin{align*}\n",
    "t_i \\sim y(x_i,\\mathbf{w}) = w_0x_i^0 + w_1x_i^1 + w_2x_i^2+\\cdots+w_Mx_i^M\n",
    "\\end{align*}\n",
    "\n",
    "* Although the polynomial function $y(x,\\mathbf{w})$ is a nonlinear function of $x$, it is a **linear function of the parameters $\\mathbf{w}$**. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called *linear models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **linear basis model** for regression takes linear combinations of deterministic nonlinear functions of the input variables\n",
    "\n",
    "\\begin{align*}\n",
    "t \\sim y(x,\\mathbf{w}) = w_0 + \\sum_{j=1}^{M} w_j\\phi_j(x)\n",
    "\\end{align*}\n",
    "\n",
    "* The parameter $w_0$ allows for any fixed offset in the data and is sometimes called a *bias* parameter. \n",
    "\n",
    "* It is often convenient to include an additional dummy *basis function* $\\phi_0(x) = 1$ so that\n",
    "\n",
    "\\begin{align*}\n",
    "t \\sim y(x,\\mathbf{w}) = \\sum_{j=0}^{M} w_j\\phi_j(x) = \\mathbf{X}\\mathbf{w}\n",
    "\\end{align*}\n",
    "where $\\mathbf{w} = \\left[w_{0}, w_{1}, \\ldots, w_M\\right]^T$, $\\mathbf{\\phi} = \\left[\\phi_{0}, \\phi_{1}, \\ldots, \\phi_M\\right]^T$ and $\\mathbf{X} = \\left[\\begin{array}{ccccc}  1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\ 1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M} \\end{array}\\right]$.\n",
    "</div>\n",
    "\n",
    "* The example of the **polynomial regression** is a particular example of this model in which there is a single input variable $x$, and the basis functions take the form of powers of $x$ so that $\\phi_j(x)=x^j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the coefficients $\\mathbf{w}$ will be determined by *fitting* the polynomial regression model to the training data (or simply **training the model**).\n",
    "\n",
    "This can be done by minimizing an **objective function** (also defined as **cost function**, **error function**, or **loss function**) that measures the *misfit* between the function $y(x,\\mathbf{w})$, for any given value of $\\mathbf{w}$, and the training set data points $\\{(x_i,t_i)\\}_{i=1}^N$.\n",
    "\n",
    "* What is the model's *objective* or goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-26T14:19:27.077Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/LeastSquares.png', width=400)\n",
    "\n",
    "#Source: Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple choice for fitting the model is to consider the error function given by the sum of the squares of the errors between the predictions $y(x_i,\\mathbf{w})$ for each data point $x_i$ and the corresponding target values $t_i$, so that we minimize\n",
    "\n",
    "\\begin{align*} J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y(x_n,\\mathbf{w})\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - \\sum_{j=0}^{M} w_jx_n^j\\right)^2\\\\\n",
    "&= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - \\phi(\\mathbf{x}_n)^T\\mathbf{w}\\right)^2\\\\\n",
    "&= \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2\\\\\n",
    "&= \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{y} \\right\\Vert^2_2\n",
    "\\end{align*}\n",
    "\n",
    "Other objective functions can be considered, for example, the absolute value of the absolute error is often used for sparsity concerns. But we will come back to this topic and discuss it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 4 - Objective Function</b> \n",
    "\n",
    "This is the measure we want to optimize, that is, minimize (if it is an error function) or maximize if it is a reward function. The objective function can take many forms, for example, the **least squares error** or **mean least squares error**.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) = \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{y} \\right\\Vert^2_2\n",
    "\\end{align*}\n",
    "\n",
    "</div>\n",
    "\n",
    "* Note that $J(\\mathbf{w})$ is a scalar value.\n",
    "\n",
    "* This objective function is minimizing the error of projection (orthogonal distance to the curve).\n",
    "\n",
    "* We can rewrite the objective function compactly in matrix/vector form:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{y} \\right\\Vert^2_2 \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2\\\\\n",
    "&= \\frac{1}{2} \\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w} \\right)^T \\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\\\\\n",
    "\\text{where } & \\mathbf{X} = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right], \\mathbf{w} =  \\left[\\begin{array}{c}\n",
    "w_{0}\\\\\n",
    "w_{1}\\\\\n",
    "\\vdots\\\\\n",
    "w_{M}\n",
    "\\end{array}\\right], \\text{and }  \\mathbf{t} = \\left[\\begin{array}{c}\n",
    "t_{1}\\\\\n",
    "t_{2}\\\\\n",
    "\\vdots\\\\\n",
    "t_{N}\n",
    "\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* Recall that $\\left\\Vert \\mathbf{z} \\right\\Vert_p$ is called the *p-norm* of the vector $\\mathbf{z}$ and is defined as $\\left(z_1^p + z_2^p + \\cdots + z_N^p\\right)^{\\frac{1}{p}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 5 - Learning Algorithm</b> \n",
    "\n",
    "We *fit* the polynomial function model such that the *objective function* $J(\\mathbf{w})$ is optimized.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) = \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2\n",
    "\\end{align*}\n",
    "\n",
    "The optimization problem is posed as:\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align*}\n",
    "\n",
    "This reads \"find the argument $\\mathbf{w}$ that minimizes the objective function $J(\\mathbf{w})$\".\n",
    "</div>\n",
    "\n",
    "* Optimizing the objective function $J(\\mathbf{w})$ is finding the *optimal* set of parameter $\\mathbf{w}^*$ that minimize the objective function.\n",
    "\n",
    "* To do that, we **take the derivative of $J(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$**.\n",
    "\n",
    "* How do you take the derivative of a *scalar*, such as $J(\\mathbf{w})$, with respect to a vector, such as $\\mathbf{w}$?\n",
    "\n",
    "    * What is the derivative of a scalar with respect to a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the scalar $J(\\mathbf{w})$ with respect to the vector $\\mathbf{w}=[w_0,w_1,\\dots,w_M]^T$ is a **vector**, and it corresponds to take the derivative of $J(\\mathbf{w})$ with respect to every element in $\\mathbf{w}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\left[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial J(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial J(\\mathbf{w})}{\\partial w_M} \\right]^T\n",
    "\\end{align*}\n",
    "\n",
    "* If we rewrite the objective function as:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left(\\mathbf{t}- \\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}- \\mathbf{X}\\mathbf{w}\\right) \\\\\n",
    "& = \\frac{1}{2} \\left(\\mathbf{t}^T - \\mathbf{w}^T\\mathbf{X}^T\\right)\\left(\\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right) \\\\\n",
    "& = \\frac{1}{2} \\left( \\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solving for $\\mathbf{w}$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[\\frac{1}{2} \\left(\\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right) \\right] &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[ \\left(\\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right) \\right] &= 0 \\\\\n",
    "- \\mathbf{t}^T\\mathbf{X} - (\\mathbf{X}^T \\mathbf{t})^T + (\\mathbf{X}^T\\mathbf{X}\\mathbf{w})^T + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &=0 \\\\\n",
    "\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} &= 0\\\\\n",
    "2 \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &= 2 \\mathbf{t}^T\\mathbf{X} \\\\\n",
    "(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X})^T &= (\\mathbf{t}^T\\mathbf{X})^T\\text{, apply transpose on both sides} \\\\\n",
    "\\mathbf{X}^T\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^T\\mathbf{t} \\\\\n",
    "\\mathbf{w} &= \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{align*}\n",
    "\n",
    "* This gives us the optimal set of parameters $\\mathbf{w}$ that minimize the objective function $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**\n",
    "\n",
    "After the model is trained (i.e. complete optimization of error function using the training labeled data), the **goal** is to *predict* the output values to *new*, unseen and unlabeled test data.\n",
    "\n",
    "The steps in the test data are:\n",
    "* Step 1: Extract (the same) features\n",
    "* Step 2: Run through the trained model using the optimal set of parameters $\\mathbf{w}$ to computer the output prediction value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9 - Conjugate Priors & The Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Read [section 2.3 \"The Gaussian Distribution\"](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) from the Bishop textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian distribution is a widely used probabilistic model for the probability density function (pdf) of continuous random variables. \n",
    "\n",
    "The Gaussian distribution can model both univariate (1-D) or multivariate (multi-dimensional) samples.\n",
    "\n",
    "In the **univariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, or, $X\\sim N(\\mu,\\sigma^2)$.\n",
    "\n",
    "* We can define the **precision** parameter $\\beta$ as the inverse of the variance, that is, $\\beta=\\frac{1}{\\sigma^2}$.\n",
    "\n",
    "* A Gaussian distribution is called **Normal** when the mean is $\\mu=0$ and variance is $\\sigma^2=1$, $X\\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1=stats.norm(0,1) # G(mean=0, variance=1^2)\n",
    "\n",
    "G2=stats.norm(10,np.sqrt(8)) #G(mean=10, variance=8)\n",
    "\n",
    "G3=stats.norm(-5,0.3) #G(mean=-5, variance=0.3^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "x=np.linspace(-8,18,1000)\n",
    "plt.plot(x,G1.pdf(x),label='Gaussian(0,1)')\n",
    "plt.plot(x,G2.pdf(x),'--',label='Gaussian(10,8)')\n",
    "plt.plot(x,G3.pdf(x),'-.',label='Gaussian(-5,0.09)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Probability Density Function \\n $f_X(x)$',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=G1.rvs(size=10_000)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "x=np.linspace(-5,5,100)\n",
    "plt.hist(samples,density=True, label='Histogram for\\n 100 samples')\n",
    "plt.plot(x, G1.pdf(x), label='pdf for G(0,1)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Density',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **multivariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}^D$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^d\\left|\\Sigma\\right|}}\\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)^T\\Sigma^{-1}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$, or, $X\\sim N(\\mu,\\Sigma)$.\n",
    "\n",
    "* $\\mu$ is a $D$-dimensional mean vector\n",
    "* $\\Sigma$ is a $D\\times D$ covariance matrix\n",
    "* $\\left|\\Sigma\\right|$ denotes the determinant of $\\Sigma$\n",
    "* The precision parameter in a $d$-dimensional space is equal to $\\beta = \\Sigma^{-1}$\n",
    "\n",
    "Let $X=[X_1,X_2]$. The **covariance** $\\Sigma$ measures the amount of variance is each individual dimension, $X_1$ and $X_2$, as well as the amount of covariance between the two. We can write the covariance as\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{cov}(X_1,X_2) &= E\\bigl[\\left(X_1-E\\left[X_1\\right]\\right) \\left(X_2-E\\left[X_2\\right]\\right)\\bigr]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\text{var}(X_1) & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\text{var}(X_2)\\end{array}\\right]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\sigma^2_{X_1} & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\sigma^2_{X_2}\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* The **Pearson's correlation coefficient** between random variables $X_1$ and $X_2$ is defined as:\n",
    "\n",
    "$$ r = \\frac{\\operatorname{cov}(X_1,X_2)}{\\sqrt{\\text{var}(X_1)}\\sqrt{\\text{var}(X_2)}} = \\frac{\\text{cov}(X_1,X_2)}{\\sigma_{X_1} \\sigma_{X_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([0,0]) # mean vector\n",
    "cov  = np.array([[2,0],[0,2]]) # covariance matrix\n",
    "print(cov)\n",
    "G = stats.multivariate_normal(mu, cov)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-6:6:100j, -6:6:100j]\n",
    "xy = np.column_stack([x.flat, y.flat])\n",
    "z = stats.multivariate_normal.pdf(xy, mean=mu, cov=cov)\n",
    "z = z.reshape(x.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x,y,z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                cmap=plt.cm.viridis)\n",
    "ax.set_xlabel('$x_1$',size=15)\n",
    "ax.set_ylabel('$x_2$',size=15)\n",
    "ax.set_zlabel('PDF $f_X(x_1,x_2)$',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Mathematica's demonstration [\"Joint Density of Bivariate Gaussian Random Variables\"](https://demonstrations.wolfram.com/JointDensityOfBivariateGaussianRandomVariables/) to better understand the role of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multivariate Gaussian-Gaussian Conjugate Prior Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a D-dimensional Gaussian data likelihood with mean $\\mu$ and covariance $\\beta\\mathbf{I}$ and a prior distribution with mean $\\mu_0$ and covariance $\\Sigma_0$\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{t}|\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}, \\beta\\mathbf{I}) \\\\\n",
    "P(\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}_0,\\Sigma_0)\n",
    "\\end{align*}\n",
    "\n",
    "The posterior distribution\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{w}|\\mathbf{t}) &\\sim \\mathcal{N}\\left(\\mathbf{\\mu}_N, \\Sigma_N\\right) \\\\\n",
    "\\mathbf{\\mu}_N &= \\Sigma_N \\left(\\Sigma_0^{-1}\\mathbf{\\mu}_0+\\beta\\mathbf{\\mathbf{X}}^T\\mathbf{t}\\right)\\\\\n",
    "\\Sigma_N^{-1} &= \\Sigma_0^{-1} + \\beta \\mathbf{\\mathbf{X}}^T\\mathbf{\\mathbf{X}}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{X}$ is the feature matrix of size $N \\times M$.\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\Sigma_0$?\n",
    "\n",
    "To simplify, let's assume the covariance of the prior to be **isotropic**, that is, it is a diagonal matrix with the same value along the diagonal, $\\Sigma_0 = \\alpha^{-1}\\mathbf{I}$. And, let $\\mathbf{\\mu}_0 = [0,0]$, thus \n",
    "\n",
    "$$\\mu_N = \\beta \\Sigma_N\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Sigma_N^{-1} = \\alpha\\mathbf{I} + \\beta \\mathbf{X}^T\\mathbf{X} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the example presented in the Bishop textbook (Figure 3.7 in page 155).\n",
    "\n",
    "Consider a single input variable $\\mathbf{x}$, a single target variable $\\mathbf{t}$ and a linear model of the form $y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1\\mathbf{x}$.\n",
    "Because this has just two parameters coefficients, $w=[w_0, w_1]^T$, we can plot the prior and posterior distributions directly in parameter space (2-dimensional parameter space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Let's generate some synthetic data from the function $f(x, a) = w_0 + w_1x$ with parameter values $w_0 = −0.3$ and $w_1 = 0.5$ by first choosing values of $x_n$ from the uniform distribution $U(x_n|−1, 1)$, then evaluating $f(x_n, \\mathbf{w})$, and finally adding Gaussian noise with standard deviation of $\\sigma = 0.2$ to obtain the target values $t_n$.\n",
    "\n",
    "$$t_n = f(x_n, \\mathbf{w}) + \\epsilon = -0.3 + 0.5 x_n + \\mathbf{\\epsilon}$$\n",
    "\n",
    "where $\\mathbf{\\epsilon}\\sim \\mathcal{N}(0,\\beta\\mathbf{I})$.\n",
    "\n",
    "* **Our goal is to recover the values of $w_0$ and $w_1$ from such data, and we will explore the dependence on the size of the data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "For some data, $\\{x_n,t_n\\}_{n=1}^N$, we can pose this problem in terms of **Regularized Least Squares**:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^1 w_i^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^2 \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\left(w_0^2 + w_1^2\\right)\\\\\n",
    "& \\Rightarrow \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align*}\n",
    "\n",
    "* Using **MAP**, we can rewrite our objective using the **Bayesian interpretation**:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{w}} \\max P(\\mathbf{\\epsilon}|\\mathbf{w})P(\\mathbf{w})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the data likelihood, $P(\\mathbf{\\epsilon}|\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma^2 = \\beta\\mathbf{I}$. And let's also consider the prior distribution, $P(\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2 = \\alpha^{-1}\\mathbf{I}$. Then, using the derivations from above, we can rewrite our optimization as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{w}} \\max & \\mathcal{N}(\\mathbf{\\epsilon}|0,\\beta\\mathbf{I})\\mathcal{N}(\\mathbf{w}|\\mathbf{\\mu_0} ,\\alpha^{-1}\\mathbf{I}) \\\\\n",
    "\\propto\\arg_{\\mathbf{w}} \\max & \\mathcal{N}\\left( \\beta \\Sigma_N^{-1} \\mathbf{X}^T\\mathbf{t}, \\Sigma_N \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{\\mu}_0 = [0,0]$, $\\mathbf{X}$ is the polynomial feature matrix, and $\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T \\mathbf{X}\\right)^{-1}$ is the covariance matrix of the posterior distribution.\n",
    "\n",
    "Note that we **do not known** the parameters of the prior distribution ($\\mu_0$ and $\\sigma_0$ are unknown). The parameters of the prior distribution will have to be chosen by the user. And they will essentially *encode* any behavior or a priori knowledge we may have about the weights.\n",
    "\n",
    "* **Both our data likelihood and prior distributions are in a 2-dimensional space (this is because our *model order* is $M=2$ -- we have 2 parameters!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate data from $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is drawn from a zero-mean Gaussin distribution.\n",
    "\n",
    "* **The goal is to estimate the values $w_0=-0.3$ and $w_1=0.5$**\n",
    "* The feature matrix $\\mathbf{X}$ can be computed using the polynomial basis functions\n",
    "* **Parameters to choose:** $\\beta$ and $\\alpha$\n",
    "\n",
    "We want to implement this scenario for a case that we are getting more data every minute. As we get more and more data, we want to **update our prior distribution using our posterior distribution (informative prior)**, because they take the have the same distribution form. This is only possible because because Gaussian-Gaussian have a conjugate prior relationship. That is, the posterior distribution is also a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "def likelihood_prior_func(beta = 2, alpha = 1, draw_num=(0,1,10,20,50,100)):\n",
    "    '''Online Update of the Posterior distribution for a Gaussian-Gaussian conjugate prior.\n",
    "    Parameter:\n",
    "    beta - variance of the data likelihood (of the additive noise)\n",
    "    alpha - precision value or 1/variance of the prior distribution\n",
    "    draw_num - number of points collected at each instance.\n",
    "    \n",
    "    This function will update the prior distribution as new data points are received.\n",
    "    The prior distribution will be the posterior distribution from the last iteration.'''\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true (unknown) weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space\n",
    "    rangeX = [-2, 2] # range of values for the input\n",
    "    step = 0.025 # distance between points\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step] # creates a grid of values for input samples\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    sigma = S0 # copying it so we can update it later\n",
    "    mean = [0,0] # mean for prior\n",
    "    \n",
    "    # Draws samples from Uniform(-1,1) distribution\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    # Generate the noisy target samples\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=np.sqrt(beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            \n",
    "            # INPUT DATA\n",
    "            #Feature Matrix (Polynomial features with M=2)\n",
    "            FeatureMatrix = np.array([draws[:draw_num[i]]**m for m in range(2)]).T\n",
    "            #Target Values\n",
    "            t = T[0:draw_num[i]]\n",
    "            \n",
    "            # POSTERIOR PROBABILITY\n",
    "            # Covariance matrix\n",
    "            sigma = np.linalg.inv(S0 + beta*FeatureMatrix.T@FeatureMatrix)\n",
    "            # Mean vector\n",
    "            mean = beta*sigma@FeatureMatrix.T@t\n",
    "            \n",
    "            # PARAMETER SPACE\n",
    "            # create a meshgrid of possible values for w's\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            \n",
    "            # Define the Gaussian distribution for data likelihood\n",
    "            p = multivariate_normal(mean=t[draw_num[i]-1], cov=beta)\n",
    "            # Initialize the PDF for data likelihood\n",
    "            out = np.empty(w0.shape)\n",
    "            # For each value (w0,w1), compute the PDF for all data samples\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "            \n",
    "            # Plot the data likelihood\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            # Add the current value for parameters w=(w0,w1)\n",
    "            ax.scatter(a,b, c='r',marker='x')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # PARAMETER SPACE\n",
    "        # create a meshgrid of possible values for w's\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        \n",
    "        # POSTERIOR PROBABILITY\n",
    "        # initialize the matrix with posterior PDF values\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        # for w0\n",
    "        pos[:, :, 0] = w0\n",
    "        # and for w1\n",
    "        pos[:, :, 1] = w1\n",
    "        # compute the PDF\n",
    "        p = multivariate_normal(mean=mean, cov=sigma)\n",
    "\n",
    "        #Show prior/posterior\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        # Add the value for parameters w=(w0,w1) that MAXIMIZE THE POSTERIOR\n",
    "        ax.scatter(a,b, c='r',marker='x')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # DATA SPACE\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "        for j in range(6):\n",
    "            # draw sample from the prior probability to generate possible values for parameters \n",
    "            w0, w1 = np.random.multivariate_normal(mean=mean, cov=sigma)\n",
    "            # Estimated labels\n",
    "            t = w0 + w1*X\n",
    "            # Show data space\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(FeatureMatrix[:,1], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# beta - variance of the data likelihood (for the additive noise)\n",
    "# alpha - precision value or 1/variance for the prior distribution\n",
    "# draw_num - number of points collected at each instance\n",
    "\n",
    "likelihood_prior_func(beta = 2, alpha = 1/2, draw_num=(0,1,2,20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification & Probabilistic Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on regression. We will begin to discuss **classification**.\n",
    "\n",
    "Suppose we have training data from two classes, $C_1$ and $C_2$, and we would like to train a classifier to assign a label to incoming test points as $C_1$ or $C_2$.\n",
    "\n",
    "There are *many* classifiers in the machine learning literature. We will cover a few in this course. Today we will focus on probabilistic generative approaches for classification.\n",
    "\n",
    "There are two types of classification algorithms: **discriminative** or **generative**.\n",
    "\n",
    "* A **discriminative** approach for classification is one in which we partition the feature space into regions for each class. Then, when we have a test point, we evaluate in which region it landed on and classify it accordingly.\n",
    "\n",
    "* A **generative** approach for classification is one in which we estimate the parameters for distributions that generate the data for each class. Then, when we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    data_C1 = stats.multivariate_normal(mean1, cov1).rvs(size=N1)\n",
    "    data_C2 = stats.multivariate_normal(mean2, cov2).rvs(size=N2)\n",
    "        \n",
    "    # Entire Training Dataset\n",
    "    data = np.concatenate((data_C1, data_C2))\n",
    "    labels = np.concatenate((np.ones(N1),2*np.ones(N2)))\n",
    "    \n",
    "    if len(mean1)>1:\n",
    "        plt.scatter(data[labels==1,0], data[labels==1,1], c='b', alpha=0.5, edgecolors='k')\n",
    "        plt.scatter(data[labels==2,0], data[labels==2,1], c='r', alpha=0.5, edgecolors='k')\n",
    "        plt.xlabel('Feature 1'); plt.ylabel('Feature 2');\n",
    "    else:\n",
    "        plt.scatter(data[labels==1], np.ones(N1), c='b', alpha=0.5, edgecolors='k')\n",
    "        plt.scatter(data[labels==2], np.ones(N2), c='r', alpha=0.5, edgecolors='k')\n",
    "        plt.xlabel('Feature 1');\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Univariate Data Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-2]\n",
    "mean2 = [1]\n",
    "var1 = [1]\n",
    "var2 = [2]\n",
    "N1 = 50 # C1 - blue\n",
    "N2 = 100 # C2 - red\n",
    "\n",
    "data, labels = generateData(mean1, mean2, var1, var2, N1, N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMAP(data, labels):\n",
    "    \n",
    "    #### Estimate parameters (MLE solution)\n",
    "    # Means\n",
    "    mu1 = np.mean(data[labels==1]) \n",
    "    mu2 = np.mean(data[labels==2])\n",
    "    \n",
    "    # Variances\n",
    "    var1 = np.cov(data[labels==1])\n",
    "    var2 = np.cov(data[labels==2])\n",
    "    \n",
    "    #### Estimate Prior Probabilities - relative frequency\n",
    "    N = len(data)\n",
    "    N1 = np.sum(labels==1)\n",
    "    N2 = N - N1\n",
    "    p1 = N1/N # prior probability for C1\n",
    "    p2 = N2/N # prior probability for C2\n",
    "    \n",
    "    #### Define data likelihoods \n",
    "    G1=stats.norm(loc=mu1,scale=np.sqrt(var1)) # P(x|C1)\n",
    "    G2=stats.norm(loc=mu2,scale=np.sqrt(var2)) # P(x|C2)\n",
    "    x=np.linspace(-6,6,1001)\n",
    "        \n",
    "    #### Plot the weighted densities\n",
    "    # these are proportional to the posteriors\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(x,p1*G1.pdf(x),label='$f_X(x|C_1)P(C_1)$')\n",
    "    plt.plot(x,p2*G2.pdf(x),label='$f_X(x|C_2)P(C_2)$')\n",
    "    \n",
    "    #### Determine the regions where the posterior for deciding C1 \n",
    "    # and the posterior for deciding C2\n",
    "    R1=x[np.where(p1*G1.pdf(x)>= p2*G2.pdf(x))]\n",
    "    R2=x[np.where(p1*G1.pdf(x)< p2*G2.pdf(x))]\n",
    "\n",
    "    # Fill under the regions found above\n",
    "    plt.fill_between(R1,p1*G1.pdf(R1),alpha=0.3,label='Decide C1')\n",
    "    plt.fill_between(R2,p2*G2.pdf(R2),alpha=0.3,label='Decide C2')\n",
    "    plt.scatter(data[labels==1], -0.01*np.ones(N1), c='b', alpha=0.5, edgecolors='k')\n",
    "    plt.scatter(data[labels==2], -0.01*np.ones(N2), c='r', alpha=0.5, edgecolors='k')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print the MAP threshold\n",
    "    print('MAP decision threshold to decide C2 is >',round(R2[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMAP(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Multivariate Data Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "data, labels = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we generated above, we have a \"red\" class and a \"blue\" class. When we are given a test sample, we will want to assign the label of red or blue.\n",
    "\n",
    "We can compute the **posterior probability** for class $C_1$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}\n",
    "\\end{align*}\n",
    "\n",
    "Understanding that the two classes, red and blue, form a partition of all possible classes, then we can utilize the *Law of Total Probability*, and obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, we can compute the posterior probability for class $C_2$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_2|x) = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}\n",
    "\\end{align*}\n",
    "\n",
    "Note that $P(C_1|x) + P(C_2|x) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Naive Bayes Classifier\n",
    "\n",
    "Therefore, for a given test point $\\mathbf{x}^*$, our decision rule is:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_1|\\mathbf{x}^*) \\underset{C_2}{\\overset{C_1}{\\gtrless}} P(C_2|\\mathbf{x}^*)\n",
    "\\end{align*}\n",
    "\n",
    "Using the Bayes' rule, we can further rewrite it as:\n",
    "\\begin{align*}\n",
    "\\frac{P(\\mathbf{x}^*|C_1)P(C_1)}{P(\\mathbf{x}^*)} &\\underset{C_2}{\\overset{C_1}{\\gtrless}} \\frac{P(\\mathbf{x}^*|C_2)P(C_2)}{P(\\mathbf{x}^*)} \\\\\n",
    "\\iff P(\\mathbf{x}^*|C_1)P(C_1) &\\underset{C_2}{\\overset{C_1}{\\gtrless}} P(\\mathbf{x}^*|C_2)P(C_2)\n",
    "\\end{align*}\n",
    "\n",
    "We assign $\\mathbf{x}^*$ as class 1 if $p(\\mathbf{x}^*|C_1) p(C_1) > p(\\mathbf{x}^*|C_2) p(C_2)$, or assign $\\mathbf{x}^*$ to class 2 if $p(\\mathbf{x}^*|C_1) p(C_1) < p(\\mathbf{x}^*|C_2) p(C_2)$.\n",
    "\n",
    "This defines the **Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Generative Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, **to train the classifier**, what we need to do is to determine the parametric forms and the associated parameters for $P(x|C_1)$, $P(x|C_2)$, $P(C_1)$ and $P(C_2)$.\n",
    "\n",
    "For example, we can assume that the data samples coming from either $C_1$ and $C_2$ are distributed according to Gaussian distributions. In this case, \n",
    "\n",
    "$$P(x|C_k) = \\frac{1}{(2\\pi)^{1/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)\\right\\}, \\forall k=\\{1,2\\}$$\n",
    "\n",
    "* We can consider any distributional form we want.\n",
    "\n",
    "What about the $P(C_1)$ and $P(C_2)$?\n",
    "\n",
    "* We can consider the relative frequency of each class, that is, $P(C_i) = \\frac{N_i}{N}$, where $N_i$ is the number of points in class $C_i$ and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Parameter Estimation Steps\n",
    "\n",
    "For simplification, let's consider the covariance matrix $\\Sigma_k$ for $k=1,2$ to be **isotropic** matrices, that is, the covariance matrix is diagonal and the element along the diagonal is the same, or: $\\Sigma_k = \\sigma_k^2\\mathbf{I}$.\n",
    "\n",
    "* What are the parameters? The mean and covariance of the Gaussian distribution for both classes.\n",
    "\n",
    "Given the assumption of the Gaussian form, how would you estimate the parameters for $p(x|C_1)$ and $p(x|C_2)$? We can use **maximum likelihood estimate** for the mean and covariance, because we are looking for the parameters of the distributions that *maximize* the data likelihood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption:** Assuming the classes follow a (bivariate or 2-D) Gaussian distribution and, for simplicity, let's assume the covariance matrices are **isotropic**, that is, $\\Sigma_k = \\sigma^2_k \\mathbf{I}$.\n",
    "\n",
    "The MLE steps for parameter estimation are:\n",
    "\n",
    "1. Write down the observed data likelihood, $\\mathcal{L}^0$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 &= P(x_1,x_2,\\dots,x_N|C_k)\\\\\n",
    "&= \\prod_{n=1}^N P(x_n|C_k),\\text{ data samples are i.i.d.} \\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x_n-\\mu_k)^T\\Sigma_k^{-1}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{d/2} |\\sigma_k^2 \\mathbf{I}|^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T\\mathbf{I}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{d/2} (\\sigma_k^2)^{d/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "where $d$ is the dimensionality of the data space.\n",
    "\n",
    "2. Take the log-likelihood, $\\mathbf{L}$. This *trick* helps in taking derivatives.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\ln\\left(\\mathcal{L}^0\\right) \\\\\n",
    "&= \\sum_{n=1}^N \\left( -\\frac{d}{2}\\ln 2\\pi - \\frac{d}{2}\\ln\\sigma_k^2 - \\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "3. Take the derivative of the log-likelihood function with respect to the parameters of interest. For Gaussian distribution they are the mean and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} &= 0\\\\\n",
    "\\sum_{n\\in C_k} \\frac{1}{\\sigma_k^2} (x_n - \\mu_k) &= 0\\\\\n",
    "\\sum_{n\\in C_k} (x_n - \\mu_k) &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - \\sum_{n\\in C_k} \\mu_k &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - N_k \\mu_k &= 0\n",
    "\\end{align*}\n",
    "\n",
    "$$\\mu_k = \\frac{1}{N_k} \\sum_{n\\in C_k} x_n$$\n",
    "\n",
    "This is the sample mean for each class. And,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_k^2} &= 0\\\\\n",
    "\\sum_{n\\in C_k} -\\frac{d}{2\\sigma_k^2} + \\frac{2(x_n - \\mu_k)^T(x_n - \\mu_k)}{(2\\sigma_k^2)^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} -d + \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=0 \\\\\n",
    "\\frac{\\sum_{n\\in C_k}(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=dN_k\n",
    "\\end{align*}\n",
    "\n",
    "$$\\sigma_k^2 = \\frac{\\sum_{n\\in C_k}(x_n - \\mu_k)^T(x_n - \\mu_k)}{dN_k}$$\n",
    "\n",
    "This is the sample variance for each class. Then we can create $\\Sigma_k = \\sigma_k^2 \\mathbf{I}$, which is the (biased) sample covariance for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, if we want to estimate an entire covariance matrix, we would have to take the derivative of the log-likelihood function with respect to every entry in the covariance matrix. Covariance matrices are symmetric, so we only need to determine the upper (or lower) half of the covariance matrix.\n",
    "\n",
    "We can determine the values for $p(C_1)$ and $p(C_2)$ from the number of data points in each class:\n",
    "\n",
    "$$p(C_k) = \\frac{N_k}{N}$$\n",
    "\n",
    "where $N$ is the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[labels==1,0], data[labels==1,1], color='red', label='C1')\n",
    "plt.scatter(data[labels==2,0], data[labels==2,1], color='blue', label='C2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Estimate parameters (with MLE solutions)\n",
    "# Means\n",
    "mu1 = np.mean(data[labels==1], axis=0)\n",
    "print('Mean of Class 1: ', mu1)\n",
    "mu2 = np.mean(data[labels==2], axis=0)\n",
    "print('Mean of Class 2: ', mu2)\n",
    "\n",
    "# Covariances - in this example we are showing the case where we estimate the full covariance\n",
    "cov1 = np.cov(data[labels==1,:].T) # np.cov expects input to be D-by-N\n",
    "print('Covariance of Class 1: ',cov1)\n",
    "cov2 = np.cov(data[labels==2, :].T)\n",
    "print('Covariance of Class 2: ',cov2)\n",
    "\n",
    "#### Estimate Prior Probabilities\n",
    "N = N1+N2\n",
    "p1 = N1/N\n",
    "print('Probability of  Class 1: ',p1)\n",
    "p2 = N2/N\n",
    "print('Probability of  Class 2: ',p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "grid = 4\n",
    "x = np.linspace(-grid, grid, 100)\n",
    "y = np.linspace(-grid, grid, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0) # grid of values\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "y1 = ## #P(x|C1) - data likelihood for C1\n",
    "y2 = ## #P(x|C2)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(data[labels==1,0], data[labels==1,1], c='r',alpha=0.3)\n",
    "plt.imshow(y1, extent=[-grid,grid,-grid,grid],cmap='YlOrRd')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(data[labels==2,0], data[labels==2,1], c='b',alpha=0.3)\n",
    "plt.imshow(y2, extent=[-grid,grid,-grid,grid], cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "pos1 = ## # P(C1|x) - posterior probability\n",
    "pos2 = ## # P(C2|x)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(pos1, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(pos2, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision boundary for deciding Class 2\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(##, extent=[-grid,grid,-grid,grid])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use this classifier to predict the class label for point $[1,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,1]\n",
    "\n",
    "# Data Likelihoods\n",
    "y1_newPoint = ## #P(x|C1)\n",
    "y2_newPoint = ## #P(x|C2)\n",
    "\n",
    "print('Data likelihoods:')\n",
    "print('P(x|C1) = ', y1_newPoint)\n",
    "print('P(x|C2) = ', y2_newPoint,'\\n')\n",
    "\n",
    "# Posterior Probabilities\n",
    "y1_pos = ## #P(C1|x)\n",
    "y2_pos = ## #P(C2|x)\n",
    "\n",
    "print('Posterior probabilities:')\n",
    "print('P(C1|x) = ', y1_pos)\n",
    "print('P(C2|x) = ', y2_pos,'\\n')\n",
    "\n",
    "if y1_pos > y2_pos:\n",
    "    print('x = ',x,' belongs to class 1')\n",
    "else:\n",
    "    print('x = ',x,' belongs to class 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about $x=[4,4]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 - Discriminant Functions, Mixture Models & Expectation-Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification can also be seen as implementing a set of **discriminant functions**, $g_i(x), i=1,\\dots, K$, such that we\n",
    "\n",
    "$$\\text{Choose} \\;\\; C_i \\;\\; \\text{if} \\;\\; g_i(x) = \\max_k g_k(x)$$\n",
    "\n",
    "where $g_i(\\mathbf{x}) = \\ln(P(C_i|\\mathbf{x})) = \\ln(P(\\mathbf{x}|C_i)P(C_i))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **two classes** ($C_1$ and $C_2$), we have the **Bayesian decision rule**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & P(C_1|x) > P(C_2|x) \\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & P(x|C_1)P(C_1) > P(x|C_2)P(C_2)\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & \\ln(P(x|C_1)P(C_1)) > \\ln(P(x|C_2)P(C_2))\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & g_1(x) > g_2(x)\\\\\n",
    "\\text{Choose} \\;\\; C_1 \\;\\; \\text{if} \\;\\; & g_1(x)-g_2(x) > 0\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **two classes**, we can define a single discriminant\n",
    "\n",
    "$$g(\\mathbf{x}) = g_1(\\mathbf{x}) - g_2(\\mathbf{x})$$\n",
    "\n",
    "and we\n",
    "\n",
    "$$\\text{Choose} \\begin{cases}C_1 & \\text{if} \\; g(\\mathbf{x})>0\\\\ C_2 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly calculate the decision boundary for the two-class two-dimensional data. Assume that the data likelihood for each class is a bivariate Gaussian distribution\n",
    "\n",
    "$$P(\\mathbf{x}|C_i) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma_i|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mu_i)^T\\Sigma_i^{-1}(\\mathbf{x}-\\mu_i)\\right\\}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mu_1 =\\begin{bmatrix}3\\\\6\\end{bmatrix}, \\;\\;\\; \\mu_2 =\\begin{bmatrix}3\\\\-2\\end{bmatrix}, \\;\\;\\; \\Sigma_1=\\begin{bmatrix}1/2 & 0\\\\0 &2\\end{bmatrix}, \\;\\;\\; \\Sigma_2=\\begin{bmatrix}2 & 0\\\\0 &2\\end{bmatrix}$$\n",
    "\n",
    "The inverse matrices are\n",
    "\n",
    "$$\\Sigma_1^{-1}=\\begin{bmatrix}2 & 0\\\\0 &1/2\\end{bmatrix}, \\;\\;\\; \\Sigma_2^{-1}=\\begin{bmatrix}1/2 & 0\\\\0 &1/2\\end{bmatrix}$$\n",
    "\n",
    "Assume equal prior probabilities $P(C_1)=P(C_2)=\\frac{1}{2}$.\n",
    "\n",
    "1. Compute the discriminant function (decision function).\n",
    "\n",
    "**Answer in board notes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = [3, 6]\n",
    "mu2 = [3, -2]\n",
    "\n",
    "Sigma1 = np.array([[0.5,0],[0,2]])\n",
    "Sigma2 = np.array([[2,0],[0,2]])\n",
    "\n",
    "p1 = 0.5\n",
    "p2 = 1-p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "gridx = 15\n",
    "gridy = 20\n",
    "x = np.linspace(-gridx, gridx, 500)\n",
    "y = np.linspace(-10, gridy, 500)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0) # grid of values\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "y1 = stats.multivariate_normal.pdf(X, mean=mu1, cov=Sigma1) #P(x|C1) - data likelihood for C1\n",
    "y2 = stats.multivariate_normal.pdf(X, mean=mu2, cov=Sigma2) #P(x|C2)\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "pos1 = (y1*p1)/(y1*p1 + y2*p2) # P(C1|x) - posterior probability\n",
    "pos2 = (y2*p2)/(y1*p1 + y2*p2) # P(C2|x)\n",
    "\n",
    "# Look at the decision boundary:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos1>pos2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-6.8,12.8, 100)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos1>pos2, extent=[-gridx,gridx,-10,gridy])\n",
    "plt.plot(x1, 3.514 - 1.125*x1 + 0.1875*x1**2, 'r', label='Discriminant function')\n",
    "plt.colorbar(); plt.legend()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the data for a *single class* looks like the plot below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples = 1500, centers = 5)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1]); plt.axis([-15,15,-15,15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a single Gaussian distribution, we would obtain a very poor estimate of the true underlying data likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Gaussian Mixture Model** or **GMM** is a probabilistic model that assumes a data likelihood to be a weighted sum of Gaussian distributions with unknown parameters.\n",
    "\n",
    "$$p(\\mathbf{x}|\\Theta) = \\sum_{k=1}^K \\pi_k N(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)$$\n",
    "\n",
    "where $\\Theta=\\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$, $0 \\leq \\pi_k \\leq 1$ and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "* When standard distributions (such as Gamma, Exponential, Gaussian, etc.) are not sufficient to characterize a *complicated* data likelihood, we can instead characterize it as the sum of weighted Gaussians distributions\n",
    "\n",
    "* Another way that GMMs are most commonly used for is to partition data in subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a Data Likelihood as a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMMs can be used to learn a complex distribution that represent a dataset. Thus, it can be used within the probabilistic generative classifier framework to model complex data likelihoods.\n",
    "\n",
    "* GMMs are also commonly used for **clustering**. Here a GMM is fit to a dataset with the goal of partitioning it into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 1</font>**\n",
    "\n",
    "Describe the **observed data likelihood**, $\\mathcal{L}^o$. As seen last class:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 2</font>**\n",
    "\n",
    "Describe the log-likelihood function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\ln\\left(\\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\\right)\\\\\n",
    "\\iff \\mathcal{L} &= \\sum_{i=1}^N \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 3</font>**\n",
    "\n",
    "Optimize for the parameters $\\Theta=\\{\\pi_k, \\mu_k,\\Sigma_k\\}_{k=1}^K$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} = 0, \\text{ and }, \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = 0\n",
    "\\end{align*}\n",
    "\n",
    "but this is a difficult problem to maximize!\n",
    "\n",
    "* A common approach for estimating the parameters of a GMM given a data set is by using the **Expectation-Maximization (EM) algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data Representation and Inference**. Represent *any* data set using a complex distribution. You may want to describe your data with a distribution but the data does not fit into a standard form (e.g. Gamma, Gaussian, Exponential, etc.), then you can use a (Gaussian) Mixture Model to describe the data. Having a representation of your data in a distribution form is a very powerful tool that, other than having a model of the data, allows you infer about new samples, make predictions, etc.\n",
    "    * Having a parametric form for a real world model, allows us to apply it in simulation and use it for designing/optimizing decision-making solutions.\n",
    "\n",
    "* **Clustering.** Partition the data into groups. Note that in the GMMs formulation we did not add the concept of labels/targets. So GMMs are an **unsupervised learning** model. It represents the data with a very complex likelihood and then we can decompose that likelihood to partition the data into categories. This is also known as **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example: Censored Data</font>**\n",
    "\n",
    "Consider the observation of i.i.d. samples $x_1, x_2, \\dots, x_N$ from the data likelihood $p(\\mathbf{x}|\\Theta)$ and we want to estimate the parameters (using MLE approach, for example)\n",
    "\n",
    "\\begin{align*}\n",
    "& \\arg_{\\Theta} \\max p(\\mathbf{x}|\\Theta) \\\\\n",
    "= & \\arg_{\\Theta} \\max \\prod_{i=1}^N p(x_i|\\Theta) \n",
    "\\end{align*}\n",
    "\n",
    "Now suppose the data samples $x_1, x_2, \\dots, x_N$ are **censored**.\n",
    "\n",
    "For example, suppose we observe i.i.d. samples, $x_1, x_2, \\dots, x_N$, from some sensor $f(\\mathbf{x})$. This sensor returns censored data in the form,\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}) = \\begin{cases} x, &&\\text{ if }  x < a \\\\ a && \\text{ if }x\\geq a \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "This means that we see $x_1, x_2,\\dots, x_m$ (less than $a$) and we do not see $x_{m+1}, x_{m+2}, \\dots, x_N$ which are censored and set to $a$.\n",
    "\n",
    "* An example of such censored data would be a scale that can only measure weight up to 120 lbs. Any measurements above 120 would be *censored* at 120.\n",
    "\n",
    "* Given this censored data, we want to estimate the mean of the data as if the data was uncensored.\n",
    "\n",
    "For this case, we can write our *observed* data likelihood as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^m p(x_i|\\Theta) \\prod_{j=m+1}^N \\int_a^{\\infty} p(x_j|\\Theta) dx_j\n",
    "\\end{align*}\n",
    "\n",
    "The data likelihood would be very difficult to maximize to solve for $\\Theta$.\n",
    "\n",
    "If only we *knew* what the missing/censored data, the problem would be easy to solve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Latent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Expectation-Maximization** or **EM** algorithm is used to find the Maximum Likelihood Estimators (MLE) (or MAP estimators) for model parameters when data is incomplete, has missing data points, or has unobserved (hidden) latent variables (such as the case of censored data). \n",
    "\n",
    "* For all of these cases, the MLE optimization is very difficult to obtain by simply taking the derivative and solve for the parameters.\n",
    "\n",
    "**<font color=orange>Step 1</font>**\n",
    "The first step of EM is to characterized the observed likelihood $\\mathcal{L}^0$.\n",
    "\n",
    "**<font color=orange>Step 2</font>**\n",
    "Introduce *hidden latent variables* (also referred to as *hidden variables*) that simplify the observed data likelihood, $\\mathcal{L}$.\n",
    "\n",
    "**<font color=orange>Step 3</font>**\n",
    "Use the hidden variables to define the *complete likelihood* $\\mathcal{L}^c$.\n",
    "\n",
    "With this, we build the EM optimization function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{z,\\Theta} \\max Q(\\Theta,\\Theta^t)\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\Theta,\\Theta^t) = E[\\ln(\\mathcal{L^c})|X,\\Theta^t]\n",
    "\\end{align*}\n",
    "\n",
    "$E[\\bullet]$ denotes expected value and $t$ denotes *iteration*. At $t=0$, we start with random values for the parameters $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this, the EM algorithm will iterate between the E-step and the M-step:\n",
    "\n",
    "1. **<font color=blue>E-step</font> (Expectation step)** Estimate the hidden variables. While holding $\\Theta$ fixed, find the variables $z$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "2. **<font color=blue>M-step</font> (Maximization step)** Estimate the parameters of the complete data likelihood $\\mathcal{L}^c$. While holding the newly found variables $z$, find the best values for the parameters $\\Theta$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "and it keeps iterating between E-step and M-step until convergence or until a certain number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EM is an **alternating optimization** algorithm, as it alternates between E-step and M-step in order to find the hidden variables and best set of parameters, respectively.\n",
    "    * In the first step ($t=0$), EM will start with a random guess for the value of the parameters $\\Theta$ in order to perform the E-step.\n",
    "    * What is the problem with alternating optimization algorithms in general?\n",
    "\n",
    "* EM is a general algorithm that can be applied to a variety of problems (not just the examples we are learning today). \n",
    "\n",
    "* EM is heavily tied with Maximum Likelihood Estimation (MLE). It is commonly used to simplify difficult MLE problems.\n",
    "    * It was originally introduced by Dempster, Laird, and Rubin in 1977 in a paper called [\"Maximum Likelihood from Incomplete Data via the EM Algorithm\"](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing GMM with the EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data likelihood for a Gaussian Mixture Model (GMM) is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What hidden variables can we add to simplify this problem?**\n",
    "\n",
    "* In this example, a hidden variable can be the label of the Gaussian from which $x_i$ was drawn from.\n",
    "\n",
    "\\begin{align*}\n",
    "z_i: \\text{label of the Gaussian from which $x_i$ was drawn from}\n",
    "\\end{align*}\n",
    "\n",
    "If we know $z_i$, then for each data point we know which Gaussian it was drawn from along with its respective parameter $\\mu_{z_i}$ and $\\Sigma_{z_i}$ and its respective weight $\\pi_{z_i}$. So, each data point would had been drawn from $\\pi_{z_i}N(x_i|\\mu_{z_i}, \\Sigma_{z_i})$. \n",
    "\n",
    "Then, assuming we have $\\{z_i\\}_{i=1}^N$, we can write the complete data likelihood:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^c = \\prod_{i=1}^N \\pi_{z_i}\\mathcal{N}(x_i|\\mathbf{\\mu}_{z_i},\\Sigma_{z_i})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate between the **E-step** and **M-step** of the EM algorithm until we find convergence or we have reached a threshold for a number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extent the optimization function:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\Theta,\\Theta^t) &= E[\\ln(\\mathcal{L^c})|X,\\Theta^t] \\\\\n",
    "&= \\sum_{\\mathbf{z}} \\ln(\\mathcal{L^c}) P(z|X,\\Theta^t) \\\\\n",
    "&= \\sum_{z_i=1}^K \\ln(\\mathcal{L^c}) P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=blue>E-step (Expectation Step)</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the E-STEP, we need to know how to compute $P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)$. \n",
    "\n",
    "* This is the posterior probability of the label $z_i$ for data sample $x_i$.\n",
    "\n",
    "* So we want to assign the label $z_i$ to the data sample $x_i$ for which the posterior probability is maximized (just like in Naive Bayes classification).\n",
    "\n",
    "Recall from **Bayes' Rule**: for two non-empty events $A$ and $B$, $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$. We can use this theorem to rewrite $P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t) &= \\frac{P(\\mathbf{x}_i|\\mathbf{z}_i,\\Theta^t)P(\\mathbf{z}_i|\\Theta^t)}{P(\\mathbf{x}_i|\\Theta^t)} \\\\\n",
    "&= \\frac{P(\\mathbf{x}_i|\\mathbf{\\mu}_{z_i}^t, \\Sigma_{z_i}^t) \\pi_{z_i}^t}{\\sum_{z_i=1}^K \\pi_{z_i}^t P(\\mathbf{x}_i|\\mathbf{\\mu}_{z_i}^t, \\Sigma_{z_i}^t)} \\\\\n",
    "&= C_{ik}\n",
    "\\end{align*}\n",
    "\n",
    "This is called the **memberships** or **responsabilities** matrix, which contains the label assignment for point $x_i$ in each Gaussian component $k.\n",
    "\n",
    "* In the E-STEP we estimate the membership matrix $C_{ik} = P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)$. This matrix is of size $N\\times K$ that contains the likelihoods of each point belonging in each one of the Gaussians.\n",
    "\n",
    "* A good check when implementing this matrix is to make sure that the sum of the rows are equal to 1!\n",
    "\n",
    "**This completes the Expectation step (E-step) in EM**. Now, we derive the update equations for the parameters $\\Theta=\\{\\pi_k,\\mu_k,\\Sigma_k\\}_{k=1}^K$  in the Maximization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=blue>M-step (Maximization Step)</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **M-step**, we are going to use (and hold constant) the membership matrix $C_{ik}$ we learned from the E-step.\n",
    "\n",
    "We will know estimate the new set of parameters $\\Theta=\\{\\pi_k,\\mu_k,\\Sigma_k\\}_{k=1}^K$ that maximize $Q(\\Theta,\\Theta^t)$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{\\Theta}}\\max Q(\\Theta,\\Theta^t)\n",
    "\\end{align*}\n",
    "\n",
    "Without loss of generality, let's assume that the covariance matrices are isotropic: $\\Sigma_k = \\sigma_k^2\\mathbf{I}$, then we can rewrite it as:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\Theta, \\Theta^t) &= \\sum_{z_i=1}^K \\ln(\\mathcal{L^c}) P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)\\\\\n",
    "&= \\sum_{z_i=1}^K \\ln\\left(\\prod_{i=1}^N \\pi_{z_i}G(x_i|\\mathbf{\\mu}_{z_i},\\Sigma_{z_i})\\right) P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t) \\\\\n",
    "&= \\sum_{k=1}^K \\ln\\left(\\prod_{i=1}^N \\pi_kG(x_i|\\mathbf{\\mu}_k,\\Sigma_k)\\right) P(\\mathbf{z}_i =k|\\mathbf{x}_i,\\Theta^t) \\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N \\left(\\ln(\\pi_k) + \\ln\\left(G(x_i|\\mathbf{\\mu}_k,\\Sigma_k)\\right) \\right) C_{ik}\\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N \\left( \\ln(\\pi_k) -\\frac{d}{2}\\ln(2\\pi) -\\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{1}{2\\sigma^2_k}\\Vert \\mathbf{x}_i -\\mu_k\\Vert_2^2 \\right) C_{ik}\n",
    "\\end{align*}\n",
    "\n",
    "So, now we want to solve:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q(\\Theta,\\Theta^t)}{\\partial \\mu_k} &= 0 \\\\\n",
    "\\frac{\\partial Q(\\Theta,\\Theta^t)}{\\partial \\sigma_k^2} &= 0 \\\\\n",
    "\\frac{\\partial Q(\\Theta,\\Theta^t)}{\\partial \\pi_k} &= 0 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
